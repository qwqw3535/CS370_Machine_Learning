{"cells":[{"cell_type":"markdown","metadata":{"id":"8mhuaIgkG65g"},"source":["---\n","# CS376 HW3: RNN, LSTM, Attention ***(Total score: 60)***\n","\n","(Note that this score is not the same score that will be used in computing your final HW grade. The score will be re-scaled considering other homeworks.)\n","\n","In this assignment, you will implement neural machine translation (NMT) models using:\n","\n","1. RNNs\n","2. LSTMs and LSTMs with attention\n","\n","As in the previous assignments, you will see code blocks that look like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 3.7\n","###############################################################################\n","pass\n","# END OF YOUR CODE\n","```\n","\n","You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 3.7\n","###############################################################################\n","x = 3.7\n","# END OF YOUR CODE\n","```\n","\n","Also, please remember:\n","- Do not write or modify any code outside of code blocks\n","- Do not add or delete any cells from the notebook (except for Discuss/Analysis section). You may add new cells to perform scatch work, but delete them before submitting.\n","- Run all cells before submitting. You will only get credit for code that has been run.\n","\n","---\n","\n","**1. How to submit**\n","* Submit **one** files to KLMS (You don't need to submit pdf file anymore):\n","  - hw3_{student_ID}.ipynb, \n","\n","**2. Note**\n","*   **Both ipynb and PDF files must contain executed log of all code blocks (after the last execution).**\n","*   Your code should be reproducible. If we cannot reproduce, you will get penalty so please make sure everything works well before you submit.\n","*   You are required to use the functions given in the notebook, do not use more advanced methods.\n","\n","\n","**3. Grading**\n","* Total score is **60pt**.\n","* Code (25pt) -  We will check whether you implemented your codes correctly and it can reproduce the results you made.\n","* Evaluation (25pt) - You can get the evalutation score whenever you pass the test cell.\n","* Discussion (10pt) - It will be given if you experiment deeply enough with different condition and provide your reasonable explanation.\n","\n","---\n","\n","###**Change log**###\n","\n","2022.04.28)  \n","  LSTM Encoder-Decoder:   LSTM encoder prediction -> LSTM decoder prediction\n"]},{"cell_type":"code","source":["# Before start, fill your information and run it.\n","\n","NAME = \"Donggyu Lee\"\n","STUDENT_ID = \"20170783\"\n","asgn3_score = 0"],"metadata":{"id":"pRHMSaFhWhOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBri01t3G65h"},"source":["## Setup\n","\n","First let's import some libraries that will be useful in this assignment.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cj9OGJRtG65h"},"outputs":[],"source":["import zipfile\n","import matplotlib.pyplot as plt\n","import random\n","import collections\n","import numpy as np\n","import zipfile\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def seed(seed):\n","  torch.manual_seed(seed)\n","  np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"lw8x1Bw7G65i"},"source":["Make sure you are using the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZJUDZ_1G65i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651996471327,"user_tz":-540,"elapsed":5,"user":{"displayName":"이동규","userId":"02845641313801357949"}},"outputId":"89390b8e-4261-4264-babe-6950c4b80273"},"outputs":[{"output_type":"stream","name":"stdout","text":["Please set GPU via Edit -> Notebook Settings.\n"]}],"source":["if torch.cuda.is_available():\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')\n","  \n","device = torch.device('cuda:0')"]},{"cell_type":"markdown","metadata":{"id":"e8hm3JYHG65j"},"source":["For this assignment, we will use an English-to-French dataset. As shown below, the dataset contains multiple lines each of which has an English sentence and its French translation separated by a tab. In this problem, since English is translated to French, English is the source language and French is the target language. Note that each text sequence is of variable lengnth and can be just one sentence or a paragraph of multiple sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J34BwlpNG65j"},"outputs":[],"source":["def download_if_not_exist(file_name):\n","  \n","  if not os.path.exists(file_name):\n","    import urllib.request\n","    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n","\n","    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n","    \n","  return file_name\n","\n","def read_raw(file_name):\n","  file_name = download_if_not_exist(file_name)\n","  \n","  with zipfile.ZipFile(file_name, 'r') as fzip:\n","    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n","  return raw_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1Td3dLnG65k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651996472730,"user_tz":-540,"elapsed":1406,"user":{"displayName":"이동규","userId":"02845641313801357949"}},"outputId":"681898f6-abf7-4657-91b6-8dac3a523ec3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Go.\tVa !\n","Run!\tCours !\n","Run!\tCourez !\n","Wow!\tÇa alors !\n","Fire!\tAu feu !\n","Help!\tÀ l'aide !\n","Jump.\tSaute.\n","Stop!\tÇa suffit !\n","Stop!\tStop !\n","Stop!\tArrête-toi !\n","Wait!\tAttends !\n","Wait!\tAttendez !\n","I see.\tJe comprends.\n"]}],"source":["raw_text = read_raw('./data.zip')\n","print(raw_text[:200])"]},{"cell_type":"markdown","metadata":{"id":"wQaSXNNXG65k"},"source":["Next we'll do some preprocessing on this raw text. We need to replace special symbols (non-breaking spaces) with spaces, convert all characters to lower case, and insert a space between words and punctuation marks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBiBxz8EG65k"},"outputs":[],"source":["def preprocess_raw(text):\n","  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n","  out = ''\n","  for i, char in enumerate(text.lower()):\n","    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n","      out += ' '\n","    out += char\n","  return out"]},{"cell_type":"markdown","metadata":{"id":"DmkUOc4gG65k"},"source":["We further split the source-target pairs into a source list and a target list. We use word-level tokenization here. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHP6ZhVGG65l"},"outputs":[],"source":["def split_source_target(text, max_len):\n","  source, target = [], []\n","  for i, line in enumerate(text.split('\\n')):\n","    if i > 5000: # we only use 5000 pairs of translation\n","      break\n","    parts = line.split('\\t')\n","    if len(parts) == 2:\n","      src_tokens = parts[0].split(' ')\n","      tgt_tokens = parts[1].split(' ')\n","      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n","        source.append(src_tokens)\n","        target.append(tgt_tokens)\n","  return source, target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1f90fA2G65l"},"outputs":[],"source":["def prepare_data(raw_text, max_len=10000):\n","  text = preprocess_raw(raw_text)\n","  source, target = split_source_target(text, max_len)\n","  return source, target\n","\n","source, target = prepare_data(raw_text)"]},{"cell_type":"markdown","metadata":{"id":"LaPsQixJG65l"},"source":["Using the whole dataset takes too much memory, and it is hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zv-uU07G65l","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1651996479598,"user_tz":-540,"elapsed":13,"user":{"displayName":"이동규","userId":"02845641313801357949"}},"outputId":"300e8b24-5203-42d7-cff2-95574a8aeced"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["def len_dis(text):\n","  lens = [len(line) for line in text]\n","  len_counter = collections.Counter(lens)\n","\n","  lens = np.array(list(len_counter.keys()))\n","  sort_idx = np.argsort(lens)\n","  lens_sort = lens[sort_idx]\n","  len_counts = np.array(list(len_counter.values()))\n","  len_counts_sort = len_counts[sort_idx]\n","  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n","  return p, lens_sort\n","  \n","src_p, src_lens_sort = len_dis(source)\n","tgt_p, tgt_lens_sort = len_dis(target)\n","plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n","plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n","plt.title('Cumulative Distribution of Sentence Length')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MpAMl-RGG65l"},"source":["From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2B-aT3qG65l"},"outputs":[],"source":["# hyper-param\n","MAX_LEN = 8\n","MIN_FREQ = 5"]},{"cell_type":"markdown","metadata":{"id":"vBrMUj8aG65m"},"source":["### Build Vocabulary\n","\n","Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n","- Dictionary word2index: Convert word string into index: \n","- Dictionary index2word: Convert index into word string\n","- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n","- Word Counter, num_word: Record the total number of unique tokens in the vocabulary \n","    \n","There are 4 special tokens added in the vocabulary:\n","- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n","- 'bos': beginning of sentence. This indicates the beginning of a sentence\n","- 'eos': end of sentence. This indicates the end of a sentence\n","- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZlC21-IG65m"},"outputs":[],"source":["class Vocab():\n","  def __init__(self, name, tokens, min_freq):\n","    self.name = name\n","    self.index2word = {\n","      0: 'pad',\n","      1: 'bos',\n","      2: 'eos',\n","      3: 'unk'\n","    }\n","    self.word2index = {v: k for k, v in self.index2word.items()}\n","    self.num_word = 4\n","    token_freq = collections.Counter(tokens)\n","    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n","    self._build_vocab(tokens)\n","    \n","  def _build_vocab(self, tokens):\n","    for token in tokens:\n","      if token not in self.word2index:\n","        self.word2index[token] = self.num_word\n","        self.index2word[self.num_word] = token\n","        self.num_word += 1\n","        \n","  def __getitem__(self, tokens):\n","    if not isinstance(tokens, (list, tuple)):\n","      return self.word2index.get(tokens, self.word2index['unk'])\n","    else:\n","      return [self.__getitem__(token) for token in tokens]"]},{"cell_type":"markdown","metadata":{"id":"MSAfZ9VYG65m"},"source":["### Build Dataset\n","\n","The dataset pipeline involves the following steps:\n","- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n","- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n","- The dataset should return the converted tensor and the corresponding valid length before padding.\n","- We use the Pytorch *DataLoader* API to build the dataset generator.\n","\n","For the purposes of this assignment, we will train and evaluate on only the training data. This isn't ideal because we do not know if we are  overfitting to the training data, but it is fine for instructional purposes. In practice (eg. for your projects), you should make sure to split your data into training/validation/test datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQzcjBZqG65m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651996484009,"user_tz":-540,"elapsed":4422,"user":{"displayName":"이동규","userId":"02845641313801357949"}},"outputId":"30cacbd8-5ef2-4d7d-9540-19b54452441e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n","Vocabulary size of source language: 433\n","Vocabulary size of target language: 420\n","Total number of sentence pairs: 4990\n"]}],"source":["def build_vocab(name, tokens, min_freq):\n","  tokens = [token for line in tokens for token in line]\n","  return Vocab(name, tokens, min_freq)\n","\n","def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n","  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n","  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n","  return vocab_src, vocab_tgt\n","\n","def pad(line, padding_token):\n","  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n","\n","def build_tensor(text, lang, is_source):\n","  lines = [lang[line] for line in text]\n","  if not is_source:\n","    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n","  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n","  valid_len = (array != lang['pad']).sum(1)\n","  return array, valid_len\n","\n","def load_data_nmt(batch_size=2):\n","  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n","  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n","  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n","  train_data = torch.utils.data.TensorDataset(\n","    src_array, src_valid_len, tgt_array, tgt_valid_len)\n","  print(train_data[0])\n","  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n","  return lang_eng, lang_fra, train_iter\n","\n","\n","source, target = prepare_data(raw_text, max_len=MAX_LEN)\n","vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n","print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n","print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n","print('Total number of sentence pairs: {}'.format(len(source)))"]},{"cell_type":"markdown","metadata":{"id":"WZuEXonbG65m"},"source":["## Sequence to Sequence with RNN (baseline)\n","\n","In this section, we provide the implementation of the seq2seq RNN baseline model. You do not need to implement any code in this section, but you should read and understand what the code is doing because you will need to implement something similar in subsequent sections. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n","\n","Implementation Details:\n","\n","- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n","- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n","- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n","- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n","- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."]},{"cell_type":"markdown","metadata":{"id":"aBNAJ6eWG65n"},"source":["<div>\n","<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/24e89824c154c2afc419c5dadec9622e490b99bb/img/seq2seq.svg\" width=\"600\"/>\n","</div>\n","Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsdusjPfG65n"},"outputs":[],"source":["class Encoder(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super(Encoder, self).__init__()\n","    \"\"\"\n","    inputs:\n","      vocab_size: int, the number of words in the vocabulary\n","      embedding_dim: int, dimension of the word embedding\n","      hidden_size: int, dimension of the hidden state of vanilla RNN\n","    \"\"\"\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n","    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n","    self.hidden_size = hidden_size\n","    \n","  def forward(self, sources, valid_len):\n","    \"\"\"\n","    Inputs:\n","      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n","      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n","    \"\"\"\n","    word_embedded = self.embedding(sources)\n","  \n","    N = word_embedded.shape[0]\n","    \n","    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n","    \n","    o, h = self.enc(word_embedded, h)\n","    \n","    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n","\n","class Decoder(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super(Decoder, self).__init__()\n","    \"\"\"\n","    inputs:\n","      vocab_size: int, the number of words in the vocabulary\n","      embedding_dim: int, dimension of the word embedding\n","      hidden_size: int, dimension of the hidden state of vanilla RNN\n","    \"\"\"\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n","    self.output_emb = nn.Linear(hidden_size, vocab_size)\n","    self.hidden_size = hidden_size\n","    \n","  def forward(self, h, target):\n","    word_embedded = self.embedding(target)\n","    N, T = word_embedded.shape[:2]\n","    \n","    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n","    pred = self.output_emb(o)\n","    return pred, h\n","\n","class NMTRNN(nn.Module):\n","  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n","    super(NMTRNN, self).__init__()\n","    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n","    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n","    \n","  def forward(self, src, src_len, tgt, tgt_len):\n","    h = self.enc(src, src_len)\n","    T = tgt.shape[1]\n","    \n","    pred, _ = self.dec(h, tgt)\n","       \n","    loss = F.nll_loss(F.log_softmax(pred[:, :T-1].transpose(1,2), dim = 1), tgt[:, 1:], ignore_index=0, reduction = 'none')\n","    loss = loss.sum(1).mean()\n","    \n","    return loss, pred.argmax(dim=-1)\n","\n","  def predict(self, src, src_len, tgt, tgt_len):\n","      \"\"\"\n","      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n","      token from the previous time step.\n","      \"\"\"\n","      h = self.enc(src, src_len)\n","\n","      inputs = tgt[:, :1]\n","      preds = []\n","      for t in range(MAX_LEN+1): # plus the 'eos' token\n","        pred, h = self.dec(h, inputs)\n","        preds.append(pred)\n","        inputs = pred.argmax(dim=-1)\n","        \n","      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n","      return pred\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbyFT1DKG65n","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"error","timestamp":1651996484673,"user_tz":-540,"elapsed":667,"user":{"displayName":"이동규","userId":"02845641313801357949"}},"outputId":"18137400-c53f-4a3c-ce4a-d3a9d033739f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-80198de11204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mrnn_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMTRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_eng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_fra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mrnn_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-80198de11204>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(net, train_iter, lr, epochs, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}],"source":["def train_rnn(net, train_iter, lr, epochs, device):\n","  # training\n","  net = net.to(device)\n","\n","  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","  loss_list = []\n","  print_interval = len(train_iter)\n","  total_iter = epochs * len(train_iter)\n","  for e in range(epochs):\n","    net.train()\n","    for i, train_data in enumerate(train_iter):\n","      train_data = [ds.to(device) for ds in train_data]\n","\n","      loss, pred = net(*train_data)\n","      \n","      loss_list.append(loss.mean().detach())\n","      optimizer.zero_grad()\n","      loss.mean().backward()\n","      optimizer.step()\n","\n","      step = i + e * len(train_iter)\n","      if step % print_interval == 0:\n","        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n","        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n","        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n","  return loss_list\n","\n","seed(1)\n","batch_size = 32\n","lr = 1e-3\n","epochs = 50\n","\n","embedding_dim = 250\n","hidden_size = 128\n","\n","vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n","rnn_net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n","\n","rnn_loss_list = train_rnn(rnn_net, train_iter, lr, epochs, device)"]},{"cell_type":"markdown","metadata":{"id":"8zy5VZYyG65o"},"source":["### RNN Loss Curve\n","\n","Plot the loss curve over time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vse6-u02G65o"},"outputs":[],"source":["# save the loss curve figure in a file for the report\n","rnn_loss_list = torch.tensor(rnn_loss_list, device = 'cpu')\n","plt.plot(np.arange(len(rnn_loss_list)), rnn_loss_list)\n","plt.title('Loss Curve of Baseline')"]},{"cell_type":"markdown","metadata":{"id":"5v1trnxiG65o"},"source":["### Prediction Accuracy\n","\n","Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset. You will see an accuracy of over 70%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLdflv6XG65o"},"outputs":[],"source":["def comp_acc(pred, gt, valid_len):\n","  N, T_gt = gt.shape[:2]\n","  _, T_pr = pred.shape[:2]\n","  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n","  len_mask = torch.arange(T_gt).expand(N, T_gt)\n","  len_mask = len_mask < valid_len[:, None]\n","  \n","  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n","  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n","  return pred_acc\n","  \n","def evaluate_rnn(net, train_iter, device):\n","  acc_list = []\n","  for i, train_data in enumerate(train_iter):\n","    train_data = [ds.to(device) for ds in train_data]\n","\n","    pred = net.predict(*train_data)\n","\n","    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n","    acc_list.append(pred_acc)\n","    if i < 5:# print 5 samples from 5 batches\n","      pred = pred[0].detach().cpu()\n","      pred_seq = []\n","      for t in range(MAX_LEN+1):\n","        pred_wd = vocab_fra.index2word[pred[t].item()] \n","        if pred_wd == 'eos':\n","          break\n","          \n","        pred_seq.append(pred_wd)\n","\n","      print('pred:\\t {}\\n'.format(pred_seq))\n","      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n","\n","  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n","  \n","seed(1)\n","batch_size = 32\n","\n","vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n","\n","evaluate_rnn(rnn_net, train_iter, device)"]},{"cell_type":"markdown","metadata":{"id":"baeu3Lk0G65o"},"source":["## Sequence to Sequence with LSTM and Attention\n","\n","Now let's try to improve our model by using an LSTM and the attention mechanism.\n"]},{"cell_type":"markdown","metadata":{"id":"7RCcKIb1G65p"},"source":["### LSTM\n","\n","LSTMs eliminate the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n","\n","$$\n","\\begin{align*}\n","&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n","&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n","&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n","&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n","&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n","&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n","\\end{align*}\n","$$\n","\n","Implement the LSTM class below. In particular,\n","-  Complete the initialization function *init_params()*. Weights should be initialized using `torch.randn` multiplied with a scale of 0.1. Biases should be initialized to 0.\n","- Complete the function *lstm()* which performs the feed-forward pass of LSTM. **Do not** use `nn.LSTM` or `nn.LSTMCell` in your implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"386a51799571153e76849c4b5ebb7f73","grade":false,"grade_id":"cell-e43516618029ca06","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wO8PeoRDG65p"},"outputs":[],"source":["class LSTM(nn.Module):\n","  def __init__(self, input_size, hidden_size, device):\n","    super(LSTM, self).__init__()\n","    self.device = device\n","    self.params = nn.ParameterList(self.init_params(input_size, hidden_size))\n","    \"\"\"\n","    Inputs:\n","      input_size: int, feature dimension of input sequence\n","      hidden_size: int, feature dimension of hidden state\n","      device: torch.device()\n","    \"\"\"\n","  \n","  def init_params(self, input_size, hidden_size):\n","    \"\"\"\n","    Inputs:\n","      input_size: int, feature dimension of input sequence\n","      hidden_size: int, feature dimension of hidden state\n","      \n","    Outputs:\n","      Weights for proposal: W_xc, W_hc, b_c\n","      Weights for input gate: W_xi, W_hi, b_i\n","      Weights for forget gate: W_xf, W_hf, b_f\n","      Weights for output gate: W_xo, W_ho, b_o\n","    \"\"\"\n","    W_xc, W_hc, b_c = None, None, None\n","    W_xi, W_hi, b_i = None, None, None\n","    W_xf, W_hf, b_f = None, None, None\n","    W_xo, W_ho, b_o = None, None, None\n","    ##############################################################################\n","    # TODO1: Initialize the weights and biases. The result will be stored in \n","    # `params` below. Weights should be initialized using `torch.randn` multiplied \n","    # with the scale (0.1). Biases should be initialized to 0.\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","    W_xc = nn.Parameter( torch.randn(input_size, hidden_size,device=self.device) * 0.1)\n","    W_xi = nn.Parameter( torch.randn(input_size,hidden_size, device=self.device) * 0.1)\n","    W_xf = nn.Parameter( torch.randn(input_size,hidden_size, device=self.device) * 0.1)\n","    W_xo = nn.Parameter( torch.randn(input_size,hidden_size, device=self.device) * 0.1)\n","    W_hc = nn.Parameter( torch.randn(hidden_size, hidden_size, device=self.device) * 0.1)\n","    W_hi = nn.Parameter( torch.randn(hidden_size, hidden_size, device=self.device) * 0.1)\n","    W_hf = nn.Parameter( torch.randn(hidden_size, hidden_size, device=self.device) * 0.1)\n","    W_ho = nn.Parameter( torch.randn(hidden_size, hidden_size, device=self.device) * 0.1)\n","    b_c = nn.Parameter( torch.zeros(hidden_size, device=self.device) )\n","    b_i = nn.Parameter( torch.zeros(hidden_size, device=self.device) )\n","    b_f = nn.Parameter( torch.zeros(hidden_size, device=self.device) )\n","    b_o = nn.Parameter( torch.zeros(hidden_size, device=self.device) )\n","\n","    # END OF YOUR CODE\n","    \n","    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n","    return params\n","\n","  \n","  def lstm(self, X, state):\n","    \"\"\"\n","    Inputs:\n","      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n","        T is the length of the sequence(s), D_in is the input size. src_len, size of (N,), is\n","        the valid length for each sequence.\n","        \n","      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of \n","            (N, hidden_size), is the memory cell of the LSTM.\n","      \n","    Outputs:\n","      o: tensor of size (N, T, hidden_size). Contains the output features (the hidden state H_t) for each t.\n","      state: the same as input state. Contains the hidden state H_T and cell state C_T for the last timestep T.\n","    \"\"\"\n","    \n","    src, src_len = X\n","    h, c = state\n","\n","    # make sure always has a T dim\n","    if len(src.shape) == 2:\n","      src = src.unsqueeze(1)\n","\n","    N, T, D_in = src.shape\n","    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n","    o = []\n","    ##############################################################################\n","    # TODO2: Implement the forward pass of the LSTM. LSTM must not be updated by\n","    # unvalid inputs. In other words, you should update LSTM only when\n","    # valid words come in. (use src_len)\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","    o = torch.zeros(N,T,h.shape[1])\n","    # print(\"N shape %d\", N)\n","    # print(\"T shape %d\", T)\n","    # print(\"h shape %d\", h.shape)\n","    src = src.to(self.device)\n","    src_len = src_len.to(self.device)\n","    h = h.to(self.device)\n","    c = c.to(self.device)\n","    \n","    new_h = torch.tensor(h)\n","    new_c = torch.tensor(c)\n","\n","    # for batch\n","    # for (i, each_batch) in enumerate(src):\n","    #   valid_len = src_len[i].int()\n","    #   for j in range(valid_len):\n","    #     input_dim = each_batch[j] \n","        \n","    #     cellgate = torch.matmul(input_dim, W_xc) + torch.matmul(h[i], W_hc) + b_c\n","    #     ingate = torch.matmul(input_dim, W_xi) + torch.matmul(h[i], W_hi) + b_i\n","    #     forgate = torch.matmul(input_dim, W_xf) + torch.matmul(h[i], W_hf) + b_f\n","    #     outgate = torch.matmul(input_dim, W_xo) + torch.matmul(h[i], W_ho) + b_o\n","\n","    #     cellgate = torch.tanh(cellgate)\n","    #     ingate = torch.sigmoid(ingate)\n","    #     forgetgate = torch.sigmoid(forgate)\n","    #     outgate = torch.sigmoid(outgate)\n","\n","    #     new_c[i] = (forgetgate * c[i]) + (ingate * cellgate)\n","    #     new_h[i] = outgate * torch.tanh(c[i])\n","    #     o[i][j] = outgate\n","    #   if(T>valid_len):\n","    #     o[i][valid_len:] = o[i][valid_len-1]\n","\n","    \n","    for j in range(T):\n","      input_dim = src[:,j,:] \n","      \n","      cellgate = torch.matmul(input_dim, W_xc) + torch.matmul(h, W_hc) + b_c\n","      ingate = torch.matmul(input_dim, W_xi) + torch.matmul(h, W_hi) + b_i\n","      forgate = torch.matmul(input_dim, W_xf) + torch.matmul(h, W_hf) + b_f\n","      outgate = torch.matmul(input_dim, W_xo) + torch.matmul(h, W_ho) + b_o\n","\n","      cellgate = torch.tanh(cellgate)\n","      ingate = torch.sigmoid(ingate)\n","      forgetgate = torch.sigmoid(forgate)\n","      outgate = torch.sigmoid(outgate)\n","\n","      new_c = (forgetgate * c) + (ingate * cellgate)\n","      new_h = outgate * torch.tanh(c)\n","      o[:,j,:] = outgate\n","    if(T>valid_len):\n","      o[:, src_len:, :] = o[:, src_len-1, :]\n","    # END OF YOUR CODE\n","\n","    state = (new_h, new_c)\n","    return o, state\n","  \n","  def forward(self, inputs, state):\n","    return self.lstm(inputs, state)"]},{"cell_type":"markdown","metadata":{"id":"2yYsrzoEG65p"},"source":["Check that your output has the correct shape. You should see:\n","\n","```\n","torch.Size([12, 8, 5])\n","torch.Size([12, 5])\n","torch.Size([12, 5])\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZladPD0NG65p"},"outputs":[],"source":["test_lstm = LSTM(10, 5, torch.device('cuda:0'))\n","test_src = torch.ones(12, 8, 10)\n","test_src_len = torch.ones(12) * 7\n","test_h = torch.zeros(12, 5).float()\n","test_c = torch.zeros(12, 5).float()\n","\n","test_o, test_state = test_lstm((test_src, test_src_len), (test_h, test_c))\n","\n","print(test_o.shape)\n","print(test_state[0].shape)\n","print(test_state[1].shape)\n","if (test_o.shape == (12,8,5) and test_state[0].shape == (12,5) and test_state[1].shape == (12,5) and torch.equal(test_o[:,6], test_o[:,7])):\n","  print(\"Correct\")\n","  LSTM_score = 5\n","\n","else:\n","  print(\"Wrong\")\n","  LSTM_score = 0"]},{"cell_type":"markdown","metadata":{"id":"g2ddJSbVG65q"},"source":["### Attention Mechanism"]},{"cell_type":"markdown","metadata":{"id":"FS_uB35eG65q"},"source":["Another improvement we can make to our model is the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."]},{"cell_type":"markdown","metadata":{"id":"kywqkQslG65q"},"source":["<div>\n","<img src=\"https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png\" width=\"600\"/>\n","</div>\n","Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"]},{"cell_type":"markdown","metadata":{"id":"4oHnD3tuG65q"},"source":["- **Attention**\n","\n","    Given a query, $\\mathbf{q} \\in R^{d_q}$, and a set of $N$ (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$ where $k_i \\in R^{d_k}$ and $v_i \\in R^{d_v}$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n","    \n","    \\begin{align*}\n","    a_i &= \\alpha(\\mathbf{q}, \\mathbf{k_i}) \\\\\n","    \\mathbf{a} &= [a_1, ..., a_n] \\\\\n","    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n","    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n","    \\end{align*}\n","    \n","    The $\\alpha()$ function, which maps two vectors into a scalar, is the score function that can be chosen from a wide range of functions: e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n"]},{"cell_type":"markdown","metadata":{"id":"4AtBw1NKG65q"},"source":["- **Masked Softmax**\n","\n","    For our machine translation task, the inputs and outputs may be of variable length (ie. each training example may have a different number of words). As shown above, we pad our inputs with a special `pad` token so that they all have the same length to make them easier to work with. However, when we take the softmax, we only want to include the non-`pad` items, so we need to write a special `masked_softmax` function to handle this. We can achieve the masking by setting masked elements to a large negative value. Then when we take the `exp`, those elements will be 0 and won't contribute to the softmax. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i85-MWhBG65q"},"outputs":[],"source":["def masked_softmax(X, valid_length):\n","  \"\"\"\n","  inputs:\n","    X: 3-D tensor\n","    valid_length: 1-D or 2-D tensor\n","  \"\"\"\n","  mask_value = -1e7 \n","\n","  if len(X.shape) == 2:\n","    X = X.unsqueeze(1)\n","\n","  N, n, m = X.shape\n","  \n","  if len(valid_length.shape) == 1:\n","    valid_length = valid_length.repeat_interleave(n, dim=0)\n","  else:\n","    valid_length = valid_length.reshape((-1,))\n","  \n","  \n","\n","  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n","  X.view(-1, m)[mask] = mask_value\n","\n","  Y = torch.softmax(X, dim=-1)\n","\n","  \n","  return Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7ZHgxnbG65q"},"outputs":[],"source":["masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"]},{"cell_type":"markdown","metadata":{"id":"DzdNXsCqG65q"},"source":["- **Scaled Dot Product Attention**\n","    - The scaled dot-product attention uses the score function as: \n","  $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d}$, where $d$ is the dimension of query (which in this case is equal to the dimension of the keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$.\n","\n","    <div>\n","    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n","    </div>\n","Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n","\n","\n","Implement the DotProductAttention below. Do not use any loops in your implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"8b7a6c1703c6c230a006a6b86326a3b9","grade":false,"grade_id":"cell-eac4fccbcd4f068e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"xNC5x7VrG65q"},"outputs":[],"source":["class DotProductAttention(nn.Module): \n","  def __init__(self):\n","      super(DotProductAttention, self).__init__()\n","\n","  def forward(self, query, key, value, valid_length=None):\n","    \"\"\"\n","    inputs:\n","      query: tensor of size (B, n, d)\n","      key: tensor of size (B, m, d)\n","      value: tensor of size (B, m, dim_v)\n","      valid_length: (B, )\n","\n","      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n","      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n","\n","    Outputs:\n","      attention: tensor of size (B, n, dim_v), weighted sum of values\n","    \"\"\"\n","    ##############################################################################\n","    # TODO3: Implement the forward pass of DotProductAttention. Do not\n","    # use any loops in your implementation.\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","    import math\n","    mm1 = torch.bmm(query, key.transpose(1,2))/math.sqrt(query.shape[2])\n","    softmax = masked_softmax(mm1, valid_length)\n","    attention = torch.bmm(softmax, value)\n","\n","    \n","    # END OF YOUR CODE\n","\n","    return attention"]},{"cell_type":"markdown","metadata":{"id":"WzXKt6chG65r"},"source":["### Correctness Check for DotProductAttention\n","\n","Run the following snippet to check your implementation of DotProductAttention.\n","\n","Expected output:\n","\n","```\n","tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n","\n","        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hkj7oVXG65r"},"outputs":[],"source":["att = DotProductAttention()\n","keys = torch.ones((2,10,2),dtype=torch.float)\n","values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n","res = att(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n","\n","ans = torch.tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],[[10.0000, 11.0000, 12.0000, 13.0000]]])\n","\n","if torch.all(torch.abs(ans - res) < 1e-03):\n","  print(\"Correct\")\n","  Dot_score = 5\n","else:\n","  print(\"Wrong\")\n","  Dot_score = 0"]},{"cell_type":"markdown","metadata":{"id":"0WG37XqrG65r"},"source":["- **MLP Attention**\n","\n","  In MLP attention, we project both query and keys into $R^h$, add the results, and use a $\\text{tanh}$ before multiplying by the values. The score function is defined as:\n","\n","    $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})$\n","    \n","    where $\\mathbf{v}, \\mathbf{W_k}\\text{, and }\\mathbf{W_q}$ are learnable parameters.\n","    \n","  Implement the MLP attention in matrix form without using any loops."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9440f3519ad5f8037f192758aecca64a","grade":false,"grade_id":"cell-6be727894d4fd817","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wIznAhcpG65r"},"outputs":[],"source":["class MLPAttention(nn.Module):  \n","  def __init__(self, d_v, d_k, d_q):\n","    super(MLPAttention, self).__init__()\n","    \"\"\"\n","    Inputs:\n","      d_k: feature dimension of key\n","      d_v: feature dimension of vector v\n","      d_q: feature dimension of query\n","    \"\"\"\n","    ##############################################################################\n","    # TODO4: Initialize learnable parameters\n","    # Use nn.Linear\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    self.W_q = nn.Linear(d_q, d_v)\n","    self.W_k = nn.Linear(d_k, d_v)\n","    self.vector_v = nn.Linear(d_v, 1)\n","\n","    # END OF YOUR CODE\n","\n","  def forward(self, query, key, value, valid_length):\n","    \"\"\"\n","    inputs:\n","      query: tensor of size (B, n, d_q)\n","      key: tensor of size (B, m, d_k)\n","      value: tensor of size (B, m, dim_v)\n","      valid_length: either (B, )\n","\n","      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n","      d_q is the feature dimension of the query, d_k is the feature dimension of the key, \n","      and dim_v is the feature dimension of the value.\n","\n","    Outputs:\n","      Y: tensor of size (B, n, dim_v), weighted sum of values\n","    \"\"\"\n","    ##############################################################################\n","    # TODO5: Implement the forward pass of MLPAttention. Do not\n","    # use any loops in your implementation.\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","    \n","    new_query = self.W_q(query)\n","    new_key = self.W_k(key)\n","    new_query = new_query.unsqueeze(2)\n","    new_key = new_key.unsqueeze(1)\n","    before_tanh = new_query + new_key\n","    after_tanh = torch.tanh(before_tanh)\n","\n","    after_v = self.vector_v(after_tanh)\n","    after_v = after_v.squeeze()\n","    after_softmax = masked_softmax(after_v, valid_length)\n","\n","    Y = torch.bmm(after_softmax, value)\n","\n","    \n","    # END OF YOUR CODE\n","    return Y"]},{"cell_type":"markdown","metadata":{"id":"4l47pb27G65r"},"source":["### Correctness Check for MLPAttention\n","\n","Run the following snippet to check your implementation of MLPAttention.\n","\n","Expected output:\n","\n","```\n","tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n","\n","        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqONoGz3G65r"},"outputs":[],"source":["atten = MLPAttention(4, 2, 2)\n","res = atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6])).detach()\n","\n","ans = torch.tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],[[10.0000, 11.0000, 12.0000, 13.0000]]])\n","\n","if torch.all(torch.abs(ans - res) < 1e-03):\n","  print(\"Correct\")\n","  MLP_score = 5\n","\n","else:\n","  print(\"Wrong\")\n","  MLP_score = 0"]},{"cell_type":"markdown","metadata":{"id":"khJX-HWEG65r"},"source":["    \n","- **Using Attention in seq2seq Models**\n","\n","<img src='https://drive.google.com/uc?id=18Z_FO69T-hS5XUltsrC4DDvoSM8LoY9y'>\n","\n","Image source: https://d2l.ai/_images/seq2seq-attention.svg\n","\n","Now we want to add attention to the seq2seq model. As we previously stated, attention allows the decoder to have more direct access to previous states in the encoder. In the context of machine translation, when the decoder is predicting a word in the translation, it can focus on certain words in the original language. Therefore, we want the keys and the values of the attention layer to be the output of the encoder at each step. The query for the attention layer would be the decoder's previous hidden state. The output of the attention layer, referred to as the context, is concatenated with the decoder input and fed into the decoder.\n","    \n","In rough pseudocode, this looks like:\n","\n","    \n","    context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n","    decoder_input = concatenate([decoder_input, context])"]},{"cell_type":"markdown","metadata":{"id":"ppM2pDCsG65r"},"source":["### LSTM Encoder-Decoder\n","\n","\n","Build a seq2seq model with LSTM and attention.\n","\n","- Complete the Encoder forward() function.\n","- Complete the Decoder forward() and predict() functions. The decoder should utilize the attention mechanism.\n","- Find a good learning rate for training this model. Feel free to add code here to test out different learning rates, but make sure that your best model is saved in `lstm_net`."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"fbd2ff0f838eab4eeb305bd07bbd3a8e","grade":false,"grade_id":"cell-85d8bda82bc92dd8","locked":false,"schema_version":3,"solution":true,"task":false},"id":"s_SKHSSkG65r"},"outputs":[],"source":["class Encoder(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n","    super(Encoder, self).__init__()\n","    \"\"\"\n","    inputs:\n","      vocab_size: int, the number of words in the vocabulary\n","      embedding_dim: int, dimension of the word embedding\n","      hidden_size: int, dimension of vallina RNN\n","    \"\"\"\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.enc = LSTM(embedding_dim, hidden_size, device)\n","    self.hidden_size = hidden_size\n","    \n","  def forward(self, sources, valid_len):\n","    ##############################################################################\n","    # TODO6: Implement LSTM Encoder forward pass\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","    \n","    #sources -> src(N, T, Din)\n","\n","    word_embedded = self.embedding(sources)\n","  \n","    N = word_embedded.shape[0]\n","    \n","    h = sources.new_zeros((N, self.hidden_size)).float() # initialize hidden state with zeros\n","    c = sources.new_zeros((N, self.hidden_size)).float() # initialize hidden state with zeros\n","    \n","    outputs, (h,c) = self.enc((word_embedded, valid_len),(h,c))\n","    \n","\n","    # END OF YOUR CODE\n","    return outputs, (h, c)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"d8f15ad91df74611a4f70744a202ad53","grade":false,"grade_id":"cell-154ce877082ed913","locked":false,"schema_version":3,"solution":true,"task":false},"id":"kg-hp8UvG65s"},"outputs":[],"source":["class Decoder(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n","    super(Decoder, self).__init__()\n","    \"\"\"\n","    inputs:\n","      vocab_size: int, the number of words in the vocabulary\n","      embedding_dim: int, dimension of the word embedding\n","      hidden_size: int, dimension of vallina RNN\n","    \"\"\"\n","    \n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.enc = LSTM(embedding_dim+hidden_size, hidden_size, device)\n","    self.att = DotProductAttention()\n","    self.output_emb = nn.Linear(hidden_size, vocab_size)\n","    self.hidden_size = hidden_size\n","    \n","  def forward(self, state, target, valid_len):\n","    loss = 0\n","    preds = []\n","    \n","    ##############################################################################\n","    # TODO7: Implement LSTM Decoder forward pass. Your solution should also use\n","    # self.att for attention.\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    # context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n","    # decoder_input = concatenate([decoder_input, context])\n","    word_embedded = self.embedding(target)\n","    N, T = word_embedded.shape[:2]\n","    \n","    outputs, (h,c), str_len = state\n","    outputs = outputs.to(device)\n","    h = h.to(device)\n","    c = c.to(device)\n","    \n","    init_att = self.att(h.unsqueeze(1), outputs, outputs, valid_len)\n","    #N, 1, Din + Dhidden   \n","    init_input = torch.cat((word_embedded[:,0,:].unsqueeze(1), init_att), dim=2)  \n","    init_output, (init_h, init_c) = self.enc((init_input, torch.ones(init_input.shape[0],1)), (h,c))\n","    pred_init = self.output_emb(init_output.to(device))\n","    \n","    # for i, each_batch in enumerate(word_embedded):\n","    #   pred_sen = pred_init[i].unsqueeze(1)\n","    #   for j in range(MAX_LEN +1):\n","        \n","    #     att = self.att(init_h[i].view(1,1,-1), outputs[i][j+1].view(1,1,-1), outputs[i][j+1].view(1,1,-1), valid_len[i])\n","    #     att = torch.cat((word_embedded[i,j+1,:].view(1,1,-1), att), dim=2)\n","        \n","    #     init_output, (init_h, init_c) = self.enc((att, torch.ones(1,1)), (init_h, init_c))\n","      \n","    #     pred_word = self.output_emb(init_output.to(device))\n","        \n","    #     pred_sen = torch.cat((pred_sen, pred_word), dim=1)\n","\n","      \n","    #   preds.append(pred_sen)\n","    \n","    \n","    \n","    for j in range(MAX_LEN +1):\n","      \n","      att = self.att(init_h.view(N,1,-1), outputs[:,j+1,:].view(N,1,-1), outputs[:,j+1,:].view(N,1,-1), valid_len)\n","      \n","      att = torch.cat((word_embedded[:,j+1,:].view(N,1,-1), att), dim=2)\n","      \n","      init_output, (init_h, init_c) = self.enc((att, torch.ones(N,1)), (init_h, init_c))\n","    \n","      pred_word = self.output_emb(init_output.to(device))\n","      \n","      pred_init = torch.cat((pred_init, pred_word), dim=1)\n","\n","    preds = pred_init\n","\n","    \n","    loss = F.nll_loss(F.log_softmax(preds[:,1:,:].transpose(1,2), dim = 1).clone(), target[:, 1:], ignore_index=0, reduction = 'none')\n","      \n","    \n","    # loss = loss.sum(1)\n","    # print(loss)\n","    # END OF YOUR CODE\n","    return loss, preds\n","  \n","  def predict(self, state, target, valid_len):\n","    pred = None\n","    ##############################################################################\n","    # TODO8: Implement LSTM Decoder prediction. Your solution should also use\n","    # self.att for attention.\n","    ##############################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    pass\n","\n","    # END OF YOUR CODE\n","\n","    return pred\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hKCKqQCG65s"},"outputs":[],"source":["class NMTLSTM(nn.Module):\n","  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n","    super(NMTLSTM, self).__init__()\n","    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n","    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n","    \n","  def forward(self, src, src_len, tgt, tgt_len):\n","    outputs, (h, c) = self.enc(src, src_len)\n","    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n","    return loss, pred\n","  \n","  def predict(self, src, src_len, tgt, tgt_len):\n","    outputs, (h, c) = self.enc(src, src_len)\n","    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n","    return pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"73bc6101f369326d21d0efe8439c652b","grade":false,"grade_id":"cell-bfaaa623c7199b2d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"mf0jjZ7GG65s"},"outputs":[],"source":["def train_lstm(net, train_iter, lr, epochs, device):\n","  # training\n","  net = net.to(device)\n","\n","  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","  loss_list = []\n","  print_interval = len(train_iter)\n","  total_iter = epochs * len(train_iter)\n","  for e in range(epochs):\n","    net.train()\n","    for i, train_data in enumerate(train_iter):\n","      train_data = [ds.to(device) for ds in train_data]\n","\n","      loss, pred = net(*train_data)\n","\n","      loss_list.append(loss.mean().detach())\n","      optimizer.zero_grad()\n","      loss.mean().backward()\n","      optimizer.step()\n","\n","      step = i + e * len(train_iter)\n","      if step % print_interval == 0:\n","        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n","        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n","        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n","  return loss_list\n","\n","seed(1)\n","batch_size = 32\n","lr = None\n","##############################################################################\n","# TODO9: Find a good learning rate to train this model. Make sure your best\n","# model is saved to the `lstm_net` variable.\n","##############################################################################\n","# Replace \"pass\" statement with your code\n","lr = 1e-3\n","# END OF YOUR CODE\n","epochs = 50\n","\n","embedding_dim = 250\n","hidden_size = 128\n","\n","vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n","lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n","\n","lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, device)"]},{"cell_type":"markdown","metadata":{"id":"M3c_DR_6G65s"},"source":["### LSTM Loss Curve\n","\n","Plot the loss curve over time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1RubQaWG65s"},"outputs":[],"source":["lstm_loss_list = torch.tensor(lstm_loss_list, device = 'cpu')\n","plt.plot(np.arange(len(lstm_loss_list)), lstm_loss_list)\n","plt.title('Loss Curve of LSTM Attention')"]},{"cell_type":"markdown","metadata":{"id":"iHBh_lS-G65s"},"source":["Test the accuracy of your model. You should be able to get at least 75% accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LzwCs7cmG65s"},"outputs":[],"source":["def comp_acc(pred, gt, valid_len):\n","  N, T_gt = gt.shape[:2]\n","  _, T_pr = pred.shape[:2]\n","  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n","  len_mask = torch.arange(T_gt).expand(N, T_gt)\n","  len_mask = len_mask < valid_len[:, None]\n","  \n","  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n","  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n","  return pred_acc\n","  \n","def evaluate_lstm(net, train_iter, device):\n","  acc_list = []\n","  for i, train_data in enumerate(train_iter):\n","    train_data = [ds.to(device) for ds in train_data]\n","\n","    pred = net.predict(*train_data)\n","\n","    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n","    acc_list.append(pred_acc)\n","    if i < 5:# print 5 samples from 5 batches\n","      pred = pred[0].detach().cpu()\n","      pred_seq = []\n","      for t in range(MAX_LEN+1):\n","        pred_wd = vocab_fra.index2word[pred[t].item()] \n","        if pred_wd == 'eos':\n","          break\n","        pred_seq.append(pred_wd)\n","\n","      print('pred:\\t {}\\n'.format(pred_seq))\n","      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n","  \n","  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n","  acc_final = torch.cat(acc_list).mean()\n","  return acc_final\n","  \n","seed(1)\n","batch_size = 32\n","\n","vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n","acc_final = evaluate_lstm(lstm_net, train_iter, device)"]},{"cell_type":"code","source":["if acc_final > 0.75:\n","  print(\"Good!\")\n","  Acc_score = 10\n","else:\n","  print(\"Try again!\")\n","  Acc_score = 0"],"metadata":{"id":"9lsIO57WKkYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total 25pt\n","asgn3_score = LSTM_score + Dot_score + MLP_score + Acc_score\n","asgn3_score"],"metadata":{"id":"2kKlkHOoLebr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Discuss and Analysis\n","\n","In this section, you can experiment with whatever RNN architecture you'd like on this translation dataset and get higher accuracy as well as possible. You may try different architectures, hyperparameters, loss functions or other things. To get full credit, you should test at least one experiment.\n","\n","Also, you should describe what you did and analyze your experiment results for each experiment. (It can be your thoughts about the limitation of your work/ a possible direction for improvement/ reason why you failed to get high accuracy,..etc)\n","\n","### Things you might try:\n","- **Hyperparameter**: Learning rate, Batch size, Vocabulary size, feature dimension, etc.\n","- **Regularization**: Dropout, Batch normalization, etc\n","- **Network architecture**: If you want, you can even use other than LSTM. (GRU,..)\n","\n","### Going above and beyond\n","If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n","\n","- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n","- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n","- Model ensembles\n","- New Architectures\n","\n","Feel free to add code blocks and explanation cells below.\n"],"metadata":{"id":"V0a-P2TnG_Df"}},{"cell_type":"markdown","metadata":{"id":"-WqXJKiMo0bn"},"source":["**Experiment #1** (10 points)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1fIazQntqtC8_WcpRkUduWDYDqZU6HKtb","timestamp":1651239899984}],"collapsed_sections":["M3c_DR_6G65s"]}},"nbformat":4,"nbformat_minor":0}